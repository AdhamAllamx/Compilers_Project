{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552f5627",
   "metadata": {},
   "source": [
    "# â—†Milestone 1: Lexical Analysis (Scanner) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ce2fe",
   "metadata": {},
   "source": [
    "### 1. Importing important Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7dde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e859a",
   "metadata": {},
   "source": [
    "### 2. Defining the Tokens types:\n",
    "2.1 Keywords = `if` , `then`,`else`,`while`,`break`,`for`,`continue`,`void`,`return`,`do`,`elif`,`switch`,`case`\n",
    "2.2 operators = `+`,`-`,`=`,`;`,`>`,`{`,`}`,`(`,`)`,`*`,`/`,`%`,`==`,`!=`,`<=`,`>=`,`&&`,`||`\n",
    "\n",
    "Note: We have added more keywords and operators as the test cases had some operators that were not included in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4daaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords = {'if','else','then','while','do','for','return','void','break','continue','elif','switch','case'}\n",
    "Operators= {'+','-','=',':',';','>','{','}','(',')','*','/','%','==','!=','<=','>=','&&','||'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663470f",
   "metadata": {},
   "source": [
    "### 3. Defining Tokens Regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97fb7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_regex = [\n",
    "    (r'\\b(if|else|then|while|do|for|return|void|break|continue|elif|switch|case)\\b','KW'),# Keywords regex \n",
    "    (r'[a-zA-Z_][a-zA-Z0-9_]*', 'ID'),# Varaible names regex small and capital letters\n",
    "    (r'\\d+', 'NUM'), # Numbers regex\n",
    "     (r'\\s+', None) ,  # added to ignore spaces\n",
    "    (r'\\+|\\-|\\*|\\/|%|==|:|!=|<=|>=|&&|\\|\\||=|;|>|\\{|\\}|\\(|\\)','OP') #Operators regex mentioned and not mentioned to be used in test case if any becuase it didnt got mentioned in the test case.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a095f1",
   "metadata": {},
   "source": [
    "### 4. Defining our Tokenization Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fa7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(line):\n",
    "    tokens_stream = []\n",
    "    curr_index = 0\n",
    "    length = len(line)\n",
    "    while curr_index < length:\n",
    "        token_found = False\n",
    "        for regex, token_type in tokens_regex:\n",
    "            regex_token=re.compile(regex)\n",
    "            token_match = regex_token.match(line, curr_index)\n",
    "            if token_match:\n",
    "                token_found = True\n",
    "                token_input=token_match.group(0)\n",
    "\n",
    "                match token_type:\n",
    "                    case 'KW':\n",
    "                        if token_input in Keywords:\n",
    "                            tokens_stream.append(f'KW({token_input})')\n",
    "                        else:\n",
    "                            raise SyntaxError(f\"Invalid Keyword: {token_input}\")\n",
    "                    case 'ID':\n",
    "                        tokens_stream.append(f'ID({token_input})')  \n",
    "                    case 'NUM': \n",
    "                        tokens_stream.append(f'NUM({token_input})')\n",
    "                    case 'OP':\n",
    "                        if token_input in Operators:\n",
    "                            tokens_stream.append(f'{token_input}')\n",
    "                        else:\n",
    "                            raise SyntaxError(f\"Invalid Operator: {token_input}\")\n",
    "                    case None:\n",
    "                        pass\n",
    "                curr_index+=len(token_input)\n",
    "                break\n",
    "        if not token_found:\n",
    "            raise SyntaxError(f\"Invalid or Unexpected character input: {line[curr_index]}\")\n",
    "    return tokens_stream\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd80107",
   "metadata": {},
   "source": [
    "### 5. Defining the Main function to Test our Tockenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c912b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(\"source_code.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                tokens = tokenizer(line)\n",
    "                print(\"Lexical Anaylizer Generated Tokens:\", tokens)\n",
    "            except SyntaxError as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d26cd8",
   "metadata": {},
   "source": [
    "### 6. Calling the Main Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecefdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Anaylizer Generated Tokens: ['ID(x)', '=', 'NUM(5)', '+', 'NUM(2)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['ID(z)', '=', 'ID(x)', '*', '(', 'ID(y)', '-', 'NUM(3)', ')', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(if)', 'ID(a)', '=', 'ID(b)', 'KW(then)', 'ID(a)', '=', 'ID(a)', '+', 'NUM(1)', ';', 'KW(else)', 'ID(b)', '=', 'ID(b)', '-', 'NUM(1)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['ID(x)', '=', 'ID(ALLAM)']\n",
      "Lexical Anaylizer Generated Tokens: ['ID(ALLAM)', '=', 'NUM(1)']\n",
      "Lexical Anaylizer Generated Tokens: ['ID(p)', '=', '(', 'ID(e)', '/', 'ID(l)', ')', '+', '(', 'ID(c)', '*', 'NUM(2)', ')', '-', 'ID(z)']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(if)', 'ID(a)', '>', 'ID(b)', '&&', 'ID(c)', '<=', 'ID(d)', '||', 'ID(e)', '!=', 'ID(f)', 'KW(then)', 'ID(x)', '=', 'ID(x)', '-', 'NUM(1)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['ID(result)', '=', '(', 'ID(x)', '+', 'ID(y)', ')', '*', '(', 'ID(a)', '-', 'ID(b)', ')', '/', 'NUM(5)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['ID(total)', '=', '(', '(', 'ID(x)', '+', 'ID(y)', ')', '-', '(', 'ID(a)', '*', 'ID(b)', ')', ')', '/', 'NUM(10)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(do)', 'ID(x)', '=', 'ID(x)', '-', 'NUM(1)', ';', 'KW(while)', 'ID(x)', '>', 'NUM(0)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(for)', 'ID(i)', '=', 'NUM(0)', ';', 'ID(i)', '<=', 'NUM(10)', ';', 'ID(i)', '=', 'ID(i)', '+', 'NUM(1)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(return)', 'ID(x)', '+', 'ID(y)', '*', 'ID(z)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(switch)', '(', 'ID(x)', ')', 'KW(case)', 'NUM(1)', ':', 'ID(y)', '=', 'ID(y)', '+', 'NUM(1)', ';', 'KW(case)', 'NUM(2)', ':', 'ID(z)', '=', 'ID(z)', '-', 'NUM(1)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(void)', 'ID(function)', '(', ')', '{', 'KW(return)', ';', '}']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(break)', ';', 'KW(continue)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(if)', 'ID(x)', '>=', 'ID(y)', '&&', 'ID(z)', '!=', 'NUM(0)', 'KW(then)', 'KW(return)', 'ID(z)', ';']\n",
      "Lexical Anaylizer Generated Tokens: ['KW(if)', 'ID(x)', '==', 'ID(y)', 'KW(then)', 'KW(return)', 'ID(x)', '*', 'ID(y)', ';']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

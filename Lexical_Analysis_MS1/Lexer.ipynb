{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552f5627",
   "metadata": {},
   "source": [
    "# â—† Milestone 1: Lexical Analysis (Scanner) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ce2fe",
   "metadata": {},
   "source": [
    "### 1. Importing important Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7dde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e859a",
   "metadata": {},
   "source": [
    "### 1. Defining the Tokens Rule Class:\n",
    "2.1 Keywords = `if` , `then`,`else`,`while`,`break`,`for`,`continue`,`void`,`return`,`do`,`elif`,`switch`,`case`\n",
    "2.2 operators = `+`,`-`,`=`,`;`,`>`,`{`,`}`,`(`,`)`,`*`,`/`,`%`,`==`,`!=`,`<=`,`>=`,`&&`,`||`\n",
    "\n",
    "Note: We have added more keywords and operators as the test cases had some operators that were not included in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4daaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenRule:\n",
    "    def __init__(self,pattern,token_type):\n",
    "        self.token_type = token_type\n",
    "        self.pattern = re.compile(pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663470f",
   "metadata": {},
   "source": [
    "### 2. Defining LexicalAnalyzer Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97fb7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalAnalyzer : \n",
    "    def __init__(self):\n",
    "        self.keywords = {'if', 'else', 'then', 'while', 'do', 'for', 'return', 'void', 'break', 'continue','elif','switch','case','print'}\n",
    "        self.operators = {'+', '-', '*', '**','/', '%', '==', ':', '!=', '<=', '>=', '&&', '||', '=', ';', '>', '{', '}', '(', ')',}\n",
    "        self.rules = [\n",
    "            TokenRule(r'\\b(if|else|then|while|do|for|return|void|break|continue|elif|switch|case|print)\\b', 'KW'), # Keywords regex\n",
    "            TokenRule(r'[a-zA-Z_][a-zA-Z0-9_]*', 'ID'), # Variable names regex small and capital letters\n",
    "            TokenRule(r'\\d+', 'NUM'), # Numbers regex   \n",
    "            TokenRule(r'\\s+', None), # added to ignore spaces\n",
    "            TokenRule(r'\\*\\*|\\+|\\-|\\*|\\/|%|==|:|!=|<=|>=|&&|\\|\\||=|;|>|\\{|\\}|\\(|\\)', 'OP')  # Operators regex\n",
    "        ]\n",
    "\n",
    "\n",
    "    def lexer(self,file_path):\n",
    "        all_tokens_stream= []\n",
    "        with open(file_path, 'r') as file:\n",
    "            line = file.readlines()\n",
    "        for line_index,line in enumerate(line,start=1):\n",
    "            curr_index = 0\n",
    "            length = len(line)\n",
    "            tokens_stream = []\n",
    "\n",
    "            while curr_index < length:\n",
    "                token_found = False\n",
    "                for rule in self.rules:\n",
    "                    match = rule.pattern.match(line,curr_index)\n",
    "                    if match:\n",
    "                        token_found= True\n",
    "                        token = match.group(0)\n",
    "\n",
    "                        match rule.token_type:\n",
    "                            case 'KW':\n",
    "                                if token in self.keywords:\n",
    "                                    tokens_stream.append(f'KW({token})')\n",
    "                            case 'ID':\n",
    "                                tokens_stream.append(f'ID({token})')  \n",
    "                            case 'NUM': \n",
    "                                tokens_stream.append(f'NUM({token})')\n",
    "                            case 'OP':\n",
    "                                if token in self.operators:\n",
    "                                    tokens_stream.append(f'{token}')\n",
    "                            case None:\n",
    "                                pass #skipping spaces\n",
    "                        curr_index+=len(token)\n",
    "                        break\n",
    "            if tokens_stream:\n",
    "                    all_tokens_stream.append(tokens_stream)\n",
    "        return all_tokens_stream\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a095f1",
   "metadata": {},
   "source": [
    "### 3. Input Scanning & Tokenization using LexicalAnalyzer `Lexer(`file_path`)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fa7cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Analyzer Tokens Stream:\n",
      "['ID(x)', '=', 'NUM(5)', '+', 'NUM(2)', ';']\n",
      "['ID(z)', '=', 'ID(x)', '*', '(', 'ID(y)', '-', 'NUM(3)', ')', ';']\n",
      "['KW(if)', 'ID(a)', '=', 'ID(b)', 'KW(then)', 'ID(a)', '=', 'ID(a)', '+', 'NUM(1)', ';', 'KW(else)', 'ID(b)', '=', 'ID(b)', '-', 'NUM(1)', ';']\n",
      "['ID(x)', '=', 'ID(ALLAM)']\n",
      "['ID(ALLAM)', '=', 'NUM(1)']\n",
      "['ID(p)', '=', '(', 'ID(e)', '/', 'ID(l)', ')', '+', '(', 'ID(c)', '*', 'NUM(2)', ')', '-', 'ID(z)']\n",
      "['KW(if)', 'ID(a)', '>', 'ID(b)', '&&', 'ID(c)', '<=', 'ID(d)', '||', 'ID(e)', '!=', 'ID(f)', 'KW(then)', 'ID(x)', '=', 'ID(x)', '-', 'NUM(1)', ';']\n",
      "['ID(result)', '=', '(', 'ID(x)', '+', 'ID(y)', ')', '*', '(', 'ID(a)', '-', 'ID(b)', ')', '/', 'NUM(5)', ';']\n",
      "['ID(total)', '=', '(', '(', 'ID(x)', '+', 'ID(y)', ')', '-', '(', 'ID(a)', '*', 'ID(b)', ')', ')', '/', 'NUM(10)', ';']\n",
      "['KW(do)', 'ID(x)', '=', 'ID(x)', '-', 'NUM(1)', ';', 'KW(while)', 'ID(x)', '>', 'NUM(0)', ';']\n",
      "['KW(for)', 'ID(i)', '=', 'NUM(0)', ';', 'ID(i)', '<=', 'NUM(10)', ';', 'ID(i)', '=', 'ID(i)', '+', 'NUM(1)', ';']\n",
      "['KW(return)', 'ID(x)', '+', 'ID(y)', '*', 'ID(z)', ';']\n",
      "['KW(switch)', '(', 'ID(x)', ')', 'KW(case)', 'NUM(1)', ':', 'ID(y)', '=', 'ID(y)', '+', 'NUM(1)', ';', 'KW(case)', 'NUM(2)', ':', 'ID(z)', '=', 'ID(z)', '-', 'NUM(1)', ';']\n",
      "['KW(void)', 'ID(function)', '(', ')', '{', 'KW(return)', ';', '}']\n",
      "['KW(break)', ';', 'KW(continue)', ';']\n",
      "['KW(if)', 'ID(x)', '>=', 'ID(y)', '&&', 'ID(z)', '!=', 'NUM(0)', 'KW(then)', 'KW(return)', 'ID(z)', ';']\n",
      "['KW(if)', 'ID(x)', '==', 'ID(y)', 'KW(then)', 'KW(return)', 'ID(x)', '*', 'ID(y)', ';']\n",
      "['KW(if)', 'ID(x)', '!=', 'NUM(0)', 'KW(then)', 'KW(print)', '(', 'ID(nonZeroDigit)', ')']\n",
      "['ID(adham_allam)', '=', 'NUM(10)', '**', 'NUM(10)', ';']\n",
      "['NUM(1)', 'ID(fxdet2)', '=', 'ID(j)', ';']\n"
     ]
    }
   ],
   "source": [
    "lexicalAnalyzer = LexicalAnalyzer()\n",
    "token_stream = lexicalAnalyzer.lexer('sample_input.txt')\n",
    "print(\"Lexical Analyzer Tokens Stream:\")\n",
    "for stream in token_stream:\n",
    "    print(stream)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

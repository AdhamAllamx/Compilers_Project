{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552f5627",
   "metadata": {},
   "source": [
    "# â—†Milestone 1: Lexical Analysis (Scanner) (Deadline: April 17th 2025 11:59 PM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ce2fe",
   "metadata": {},
   "source": [
    "### 1. Importing important Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7dde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e859a",
   "metadata": {},
   "source": [
    "### 2. Defining the Token types:\n",
    "2.1 Keywords = `if` , `then`,`else`\n",
    "2.2 operators = `+`, `-`, `*`, `=`, `>`, `;`, `{`, `}`, `(`, `)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4daaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords = {'if','else','then'}\n",
    "Operators= {'+','-','=',';','>','{','}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663470f",
   "metadata": {},
   "source": [
    "### 3. Defining Token Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_regex = [\n",
    "    (r'[a-zA-Z_][a-zA-Z0-9_]*', 'ID'),# Varaible names regex small and capital letters\n",
    "    (r'\\d+', 'NUM'), # Numbers regex\n",
    "     (r'\\s+', None) ,  # added to ignore spaces\n",
    "    (r'\\b(if|else|then)\\b','KW'),# Keywords regex \n",
    "    (r'[\\+\\-\\=\\;\\>\\<\\{\\}]','OP') #Operators regex mentioned \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a095f1",
   "metadata": {},
   "source": [
    "### 4. Defining our Tokenization Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(line):\n",
    "    tokens_stream = []\n",
    "    curr_index = 0\n",
    "    length = len(line)\n",
    "    while curr_index < length:\n",
    "        token_found = False\n",
    "        for regex, token_type in tokens_regex:\n",
    "            regex_token=re.compile(regex)\n",
    "            token_match = regex_token.match(line, curr_index)\n",
    "            if token_match:\n",
    "                token_found = True\n",
    "                token_input=token_match.group(0)\n",
    "                if token_type=='KW':\n",
    "                    if token_input in Keywords:\n",
    "                        tokens_stream.append(f'KW({token_input})')\n",
    "                elif token_type=='OP':\n",
    "                    if token_input in Operators:\n",
    "                        tokens_stream.append(f'OP({token_input})')\n",
    "                elif token_type=='ID':\n",
    "                    tokens_stream.append(f'ID({token_input})')  \n",
    "                elif token_type=='NUM': \n",
    "                    tokens_stream.append(f'NUM({token_input})')\n",
    "    while line:\n",
    "        match = None\n",
    "        \n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
